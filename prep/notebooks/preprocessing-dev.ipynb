{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateColumns:\n",
    "    TIMESTAMP_COLUMN = \"order_purchase_timestamp\"\n",
    "    DATE_COLUMN = \"purchase_date\"\n",
    "    \n",
    "class DataRoot:\n",
    "    ROOT = \"data\"\n",
    "    ZONE = \"raw\"\n",
    "    ORDERS = \"olist_orders_dataset.parquet\"\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def return_root(cls):\n",
    "        return os.path.join(cls.ROOT, cls.ZONE)\n",
    "\n",
    "    @classmethod\n",
    "    def return_orders_path(cls):\n",
    "        return os.path.join(cls.return_root(), cls.ORDERS)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orders_with_dates(orders_path: str) -> Tuple[List[datetime], pd.DataFrame]:\n",
    "    df = pd.read_parquet(orders_path)\n",
    "    df.loc[:, DateColumns.DATE_COLUMN] = pd.to_datetime(df[DateColumns.TIMESTAMP_COLUMN]).dt.date\n",
    "    all_dates = df[DateColumns.DATE_COLUMN].sort_values().unique().tolist()\n",
    "    return all_dates, df[[\"order_id\", DateColumns.DATE_COLUMN]]\n",
    "\n",
    "def add_date_column(df, orders_with_date):\n",
    "    if not DateColumns.DATE_COLUMN in df.columns:\n",
    "        print(\"Adding date columns to Dataframe...\")\n",
    "        df = df.merge(orders_with_date, how=\"inner\", on=\"order_id\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validating get_orders_with_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(634, 634)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates, orders_with_date = get_orders_with_dates(DataRoot.return_orders_path())\n",
    "df_list = [orders_with_date[orders_with_date[DateColumns.DATE_COLUMN] == d] for d in dates]\n",
    "\n",
    "len([x for x in df_list if len(x) > 0]), len(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revoming dates without payments or items\n",
    "\n",
    "There are 18 days with orders but no items. And one day with an order without payment. \n",
    "I will remove those orders otherwise the future DAG will always fails at those dates as I don't have way to find the missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding date columns to Dataframe...\n",
      "Adding date columns to Dataframe...\n",
      "Total days that remain 615\n"
     ]
    }
   ],
   "source": [
    "def filter_days_without_items_and_payments(dates):\n",
    "    items = (\n",
    "    pd.read_parquet(os.path.join(DataRoot.return_root(),  \"olist_order_items_dataset.parquet\"))\n",
    "    .pipe(add_date_column, orders_with_date)\n",
    "    )\n",
    "\n",
    "\n",
    "    payments = (\n",
    "        pd.read_parquet(os.path.join(DataRoot.return_root(),  \"olist_order_payments_dataset.parquet\"))\n",
    "        .pipe(add_date_column, orders_with_date)\n",
    "    )\n",
    "\n",
    "\n",
    "    days_without_items = set(dates).difference(set(items.purchase_date))\n",
    "    days_without_payments = set(dates).difference(set(payments.purchase_date))\n",
    "    days_without_items_or_payments = days_without_items.union(days_without_payments)\n",
    "    filtered_dates = list(set(dates).difference(days_without_items_or_payments))\n",
    "    return filtered_dates\n",
    "\n",
    "print(\"Total days that remain\", len(filter_days_without_items_and_payments(dates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dates to chunk data\n",
      "Adding date columns to Dataframe...\n",
      "Adding date columns to Dataframe...\n",
      "Start chunking the data...\n",
      "Creating chunks for olist_order_items_dataset.parquet\n",
      "Adding date columns to Dataframe...\n",
      "Removing origin dataset...\n",
      "Creating chunks for olist_order_payments_dataset.parquet\n",
      "Adding date columns to Dataframe...\n",
      "Removing origin dataset...\n",
      "Creating chunks for olist_orders_dataset.parquet\n",
      "Adding date columns to Dataframe...\n",
      "Removing origin dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting dates to chunk data\")\n",
    "dates, orders_with_date = get_orders_with_dates(DataRoot.return_orders_path())\n",
    "dates = filter_days_without_items_and_payments(dates)\n",
    "\n",
    "\n",
    "datasets_to_chunks = [\n",
    "    \"olist_order_items_dataset.parquet\",\n",
    "    \"olist_order_payments_dataset.parquet\",\n",
    "    \"olist_orders_dataset.parquet\"\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Start chunking the data...\")\n",
    "for dataset in datasets_to_chunks:\n",
    "    print(f\"Creating chunks for {dataset}\")\n",
    "    \n",
    "    complete_path = os.path.join(DataRoot.return_root(), dataset)\n",
    "    folder = complete_path.split(\".\")[0]\n",
    "    # read_df and create generator:\n",
    "\n",
    "    df = (\n",
    "            pd.read_parquet(complete_path)\n",
    "            .pipe(add_date_column, orders_with_date)\n",
    "        )\n",
    "\n",
    "    df_list = (df[df[DateColumns.DATE_COLUMN] == d] for d in dates)\n",
    "\n",
    "    # save each chunk file within dataset folder\n",
    "    for i, df_ in enumerate(df_list):\n",
    "\n",
    "        if len(df_[DateColumns.DATE_COLUMN]) == 0:\n",
    "            print(dates[i])\n",
    "            raise Exception(\"No rows for date\")\n",
    "\n",
    "        date_str = df_[DateColumns.DATE_COLUMN].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        filename = dataset.split(\".\")[0] + \"_\"  + date_str + \".parquet\"\n",
    "        df_.to_parquet(os.path.join(folder, filename))\n",
    "    \n",
    "    print(\"Removing origin dataset...\")\n",
    "    os.remove(complete_path)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d4df664cc036e824783d04b32a643e6e20611153740fb1d0bd7b0030573a6d5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('portfolio-089aDPC9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
