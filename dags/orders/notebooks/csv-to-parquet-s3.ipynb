{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_root(file, root):\n",
    "    \"\"\"\n",
    "    Apend root to path\n",
    "    \"\"\"\n",
    "    return os.path.join(root,file)\n",
    "\n",
    "\n",
    "def save_to_buffer_as_parquet(df):\n",
    "    out_buffer =  BytesIO()\n",
    "    df.to_parquet(out_buffer, index=False)\n",
    "    return out_buffer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3BucketConnector:\n",
    "    def __init__(self, aws_access_key, aws_secret, bucket_name, region_name = \"us-east-1\"):\n",
    "        self.region_name = region_name\n",
    "        self.session = boto3.Session(os.environ[aws_access_key], os.environ[aws_secret], region_name=self.region_name)\n",
    "        self._s3 = self.session.resource(\"s3\")\n",
    "        self._bucket = self._s3.Bucket(bucket_name)\n",
    "\n",
    "    def read_parquet_to_dataframe(self, key: str):\n",
    "        \"\"\"\n",
    "        Read a parquet file stored in S3 and return a dataframe\n",
    "        \"\"\"\n",
    "        print(f\"Reading file {key} from {self._bucket.name} in {self.region_name}\")\n",
    "        obj = self._bucket.Object(key=key).get().get('Body').read()\n",
    "        if not obj:\n",
    "            raise Exception(f\"{key} does not exist\")\n",
    "        data = BytesIO(obj)\n",
    "        df = pd.read_parquet(data)\n",
    "        return df\n",
    "\n",
    "    def write_df_to_s3_as_parquet(self, df: pd.DataFrame, key: str):\n",
    "        \"\"\"\n",
    "        Write a pandas DF as parquet\n",
    "        \"\"\" \n",
    "        out_buffer =  BytesIO()\n",
    "        df.to_parquet(out_buffer, index=False)\n",
    "        print(f\"Writing file {key} to {self._bucket.name} in {self.region_name}\")\n",
    "        self._bucket.put_object(Body=out_buffer.getvalue(), Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file olist/one-run/raw/olist_closed_deals_dataset.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n",
      "Writing file olist/one-run/raw/olist_customers_dataset.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n",
      "Writing file olist/one-run/raw/olist_geolocation_dataset.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n",
      "Writing file olist/one-run/raw/olist_orders_dataset.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n",
      "Writing file olist/one-run/raw/olist_order_items_dataset.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n",
      "Writing file olist/one-run/raw/olist_order_payments_dataset.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n",
      "Writing file olist/one-run/raw/olist_order_reviews_dataset.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n",
      "Writing file olist/one-run/raw/olist_products_dataset.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n",
      "Writing file olist/one-run/raw/olist_sellers_dataset.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n",
      "Writing file olist/one-run/raw/product_category_name_translation.parquet to slafaurie-airflow in us-east-1\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"..\\\\data\\\\raw\"\n",
    "\n",
    "bucket_name = \"slafaurie-airflow\"\n",
    "slafaurie_airflow_bucket = S3BucketConnector(\"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", bucket_name)\n",
    "\n",
    "prefix = \"olist/one-run/raw/\"\n",
    "for filename in os.listdir(DATA_ROOT):\n",
    "    df = pd.read_csv(add_root(filename, DATA_ROOT))\n",
    "    key = prefix + filename.split(\".\")[0] + \".parquet\"\n",
    "    slafaurie_airflow_bucket.write_df_to_s3_as_parquet(df, key)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6d1a0ffc3a81bf4a66a8640965af3b38812950f48e175d6d6ccbaf96aa1fca0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
